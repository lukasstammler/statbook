```{r setup-06, include=FALSE}
rm(list = ls())

knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.align = "center",
                      fig.dim = c(5, 4))

library(openintro)
library(tidyverse)
library(knitr)
library(patchwork)
library(Hmisc)       # for errorbars in ggplot
library(kableExtra)
library(graphics)

ggred <- "#F8766D"
ggblue <- "#00BFC4"
ggreen <- "#7CAE00"
ggviolet <- "#C77CFF"

library(scales)
show_col(COL[1:20])
```

```{r}
#knitr::write_bib(c(.packages(), "openintro"), "packages.bib")
```


# Einfache lineare Regression

Hat man einen linearen Zusammenhang zwischen zwei Variablen entdeckt, möchte man diesen Zusammenhang präzise beschreiben und quantifizieren. Das Ziel der Regressionsanalyse ist es, die Gerade anzugeben, die den linearen Zusammenhang am besten beschreibt. Im Unterschied zur Korrelation wird hier ein gerichteter Zusammenhang untersucht. Wir möchten eine Variable y, in unserem Beispiel der Kalorienverbrauch, durch eine Variable x, die Laufgeschwindigkeit vorhersagen.

## Lernziele

1. Definiere die erklärende Variable als unabhängige Variable (Prädiktor) und die Antwortvariable als abhängige Variable.  
2. Erstelle Streudiagramme so, dass die unabhängige Variable auf der x-Achse und die abhängige Variable auf der y-Achse liegt.   
3. Wenn x die unabhängige und y die abhängige Variable ist, wird das lineare Regressionsmodell gebildet als  
$$y = \beta_0 + \beta_1x$$
   wobei der Koeffizient $\beta_0$ den Schnittpunkt mit der y-Achse (Achsenabschnitt, engl. intercept) und der Koeffizient $\beta_1$ die Steigung der Geraden beschreibt. Die Punktschätzungen (aus den beobachteten Daten) für $\beta_0$ und $\beta_1$ sind $b_0$ bzw. $b_1$.   
4. Definiere Residuen $e$ als Differenz zwischen den beobachteten $y$ und den durch das Modell vorhergesagten (gefitteten) $\hat{y}$ Werten der abhängigen Variablen.  
5. Definiere die Kleinstquadratlinie (Regressionsgerade) als die Linie, welche die Summe der quadrierten Residuen minimiert. Du kannst die Bedingungen dafür nennen, dass die Konfidenzintervalle und Hypothesentests für die Koeffizienten der Kleinstquadratlinie gültig sind:   
   a) Linearität   
   b) Normalverteilung der Residuen   
   c) Konstante Variabilität (Homoskedastizität)   
6. Interpretiere die Steigung $b_1$ wie folgt      
    - "Für jede Einheit um die sich der Wert x erhöht, erwarten wir, dass y im Durchschnitt um $|b_1|$ Einheiten grösser bzw. kleiner wird."   
    - Beachte dass es vom Vorzeichen von $b_1$ abhängig ist, ob der Wert der abhängigen Variable zu- oder abnimmt.   
7. Beachte, dass die Kleinstquadratlinie stets durch den Mittelwert der abhängigen $\bar{y}$ und der unabhängigen Variable $\bar{x}$ verläuft.  
8. Interpretiere $b_0$ (intercept) folgendermassen: "Wenn x = 0, erwarten wir dass y im Durchschnitt den Wert von $b_0$ annimmt."  
9. Berechne den Wert der abhängigen Variablen für einen bestimmten Wert der unabhängigen Variablen, $x^*$, durch Einsetzen von $x^*$ in das lineare Modell:   
$$\hat{y} = b_0 + b_1x^*$$  
    - Verwende nur Werte für $x^*$, die im Bereich der beobachteten Daten liegen.  
    - Extrapoliere nicht über die Variationsbreite hinaus, ausser du bist dir sicher, dass das lineare Muster darüber hinaus gültig ist.   
10. Definiere das Bestimmtheitmass $R^2$ als prozentualen Anteil der Variabilität der abhängigen Variablen, der durch die unabhängige Variable erklärt wird.   
    - Für ein gutes Modell erwarten wir, dass dieser Wert nahe bei 1 (100%) liegt.  
    - Das Bestimmtheitsmass wird berechnet als das Quadrat des Korrelationskoeffizienten nach Pearson: $R^2 = r^2$ 
11. Entscheide anhand des Outputs im Statistikprogramm (t-Wert und p-Wert), ob die unabhängige Variable ein signifikanter Prädiktor für die abhängige Variable ist.   

## Eine Bücherbestellung

Ein Buchhändler muss jeweils einen Monat bevor das Semester beginnt die Statistik-Bücher für die Studierenden im Statistik-Kurs bestellen. Er geht davon aus, dass die Anzahl Statistik-Bücher, die er in diesem Semester verkaufen wird, davon abhängt, wieviele Studierende für den Statistik-Kurs angemeldet sind. Aus den vergangenen 12 Semestern besitzt der Buchhändler die Listen mit der Anzahl der eingeschriebenen Studierenden und mit der Anzahl der pro Semester verkauften Bücher. (Datensatz `06_bookstore.csv`)

```{r bookstore-tab}
# https://www.nku.edu/~statistics/Simple_Linear_Regression.htm
bookstore <- read_csv("./data/06_bookstore.csv")

bookstore %>% 
  kable(align = "c", caption = "Buchhandlung") %>% 
  kable_classic(bootstrap_options = "striped", full_width = FALSE, position = "center")
```

Für das kommende Semester haben sich 33 Studierende für den Statistikkurs angemeldet. Um möglichst nicht zu viele oder zu wenige Bücher zu bestellen, bittet er uns um Hilfe.  

## Unabhängige und abhängige Variable

Im vorliegenden Fall können wir davon ausgehen, dass ein kausaler Zusammenhang zwischen der Anzahl der Studierenden und der Anzahl der verkauften Bücher vorliegt: Der Wert der Variablen `Studierende` erlaubt eine Vorhersage über den Wert der Variablen `Buecher` oder m.a.W. der Wert der Variablen `Buecher` hängt vom Wert der Variablen `Studierende`ab. Damit können wir die Variable `Buecher`als **abhängige Variable** und die Variable `Studierende` als **unabhängige Variable** bzw. als **Prädiktor** bezeichnen.   
<br>

### Zusammenhang zwischen den Variablen

Wie bereits gewohnt, formulieren wir zuerst die Hypothesen:  

* $H_0: \rho = 0$ Es gibt keinen Zusammenhang zwischen der Anzahl Studierender und der verkauften Anzahl Bücher.   
* $H_A: \rho \neq 0$ Es gibt einen Zusammenhang zwischen der Anzahl Studierender und der verkauften Anzahl Bücher.    

Als nächstes erstellen wir ein Streudiagramm: Wenn ein kausaler Zusammenhang vermutet wird, wird die unabhängige Variable auf der x-Achse und die abhängige Variable auf der y-Achse dargestellt.     

```{r bookstore-fig, fig.align='center', fig.dim=c(5, 4), fig.cap="Zusammenhang Anzahl verkaufte Bücher und Anzahl Studierende"}
ggplot(bookstore, aes(x = Studierende, y = Buecher)) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  #geom_smooth(method = "lm", se = FALSE, color = "darkgrey", alpha = .5) +
  theme_grey()
```

Die Daten zeigen einen moderaten bis starken positiven linearen Zusammenhang zwischen der abhängigen und der unabhängigen Variablen. Mit der Berechnung des Korrelationskoeffizienten können wir die Stärke des Zusammenhangs quantifizieren (das Signifikanzniveau legen wir auf $\alpha = 0.05$ fest): 

```{r bookstore-cor, echo=TRUE}
### R-Code

cor.test(bookstore$Studierende, bookstore$Buecher)
```

*Die Daten liefern Evidenz für einen signifikanten, starken, positiven und linearen Zusammenhang zwischen der Anzahl an Studierenden und der Anzahl verkaufter Bücher ($r$ = 0.918, p < 0.001)*

## Einfache lineare Regression

Mit der Korrelation konnten wir die Annahme eines Zusammenhangs zwischen Anzahl Studierender und der Anzahl verkaufter Bücher bestätigen. Die Frage ist allerdings nicht beantwortet, wieviele Bücher der Buchhändler für dieses Semester, an dem 33 Studierende für den Statistik-Kurs eingeschrieben sind, bestellen muss. Es wäre ideal, wenn wir auf Grundlage der vorliegenden Daten ein funktionelles Modell erstellen könnten, das uns bei der Schätzung der Anzahl Bücher helfen würde.  

Die Regressionsanalyse liefert das Werkzeug dafür: Sie liefert uns ein Modell - nämlich eine Gerade und die zugehörige Gleichung - welches unsere Daten so gut wie möglich beschreibt. Im vorliegenden Fall hilft uns die Gleichung vorherzusagen, wieviele Bücher wir für jede zusätzliche Studierende verkaufen werden.


### Die Regressionsgerade

**Kurzes Repe Mathematik - Lineare Funktion: Geradengleichung**   

- Eine lineare Funktion kann grafisch durch eine Gerade dargestellt werden.  
- Die allgemeine Formel für eine Gerade im zweidimensionalen Koordinatensystem lautet: $y = ax + b$.  

   $a$ = Steigung: Um wieviel steigt y, wenn x um 1 Einheit grösser wird?   
   $b$ = Achsenabschnitt: Wo schneidet die Gerade die y-Achse wenn x = 0?  
   
```{r geradengleichung-fig, fig.dim=c(5, 8), fig.cap="Geradengleichung"}
x <- seq(-3, 2, by = .1)
y <- 2*x + 4

fkt <- tibble(x = x, y = y)

ggplot(fkt, aes(x = x, y = y)) +
  geom_smooth(method = "lm", color = "red") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  scale_y_continuous(breaks = seq(-3, 8)) +
  scale_x_continuous(breaks = seq(-3, 2)) +
  geom_segment(aes(x = 0, xend = 1, y = 4, yend = 4), color = "steelblue", data = fkt) +
  geom_segment(aes(x = 1, xend = 1, y = 4, yend = 6), color = "steelblue", data = fkt) +
  geom_segment(aes(x = -3, xend = 0, y = 4, yend = 4), color = "steelblue", data = fkt, linetype = "dashed") +
  theme_grey() +
  geom_text(aes(x = 0.5, y = 3.5, label = "1"), color = "steelblue", size = 6) +
  geom_text(aes(x = 1.2, y = 5, label = "2"), color = "steelblue", size = 6) +
  geom_label(aes(x = -1, y = 3.5, label = "y = 2x + 4"), color = "red") +
  geom_label(aes(x = 1.5, y = 3.5, label = "a = dy/dx = 2/1"), color = "steelblue") +
  geom_label(aes(x = -.2, y = 4.5, label = "b"), color = "steelblue")
```

Wir sehen, dass wenn x um eine Einheit zunimmt ($\Delta x$), nimmt y um zwei Einheiten zu ($\Delta y$), d.h.

$$a = \frac{\Delta y}{\Delta x} = \frac{2}{1} = 2$$

Wenn x = 0 ist y = 4, d.h. $b = 4$.  

Unsere Geradengleichung lautet somit:

$$y = 2x + 4$$


### Lineares Modell  

Lineare Regression ist die statistische Methode, eine Gerade für zwei Variablen X und Y zu konstruieren, wenn die Beziehung dieser beiden Variablen einigermassen linear ist, also durch eine Gerade beschrieben werden kann. Die Gerade dient als Modell und wird so konstruiert, dass sie die Daten möglichst passend beschreibt. Die allgemeine Formel für ein lineares Modell lautet:  

$$y = \beta_0 + \beta_1x$$

$\beta_0$ und $\beta_1$ sind die Parameter des Modells. Diese werden in der Regel mit Stichprobendaten geschätzt und wir schreiben wir die Formel als

$$y = b_0 + b_1x$$

- $b_0$ Achsenabschnitt, gibt den Wert für y an, wenn x = 0 (in Statistikprogrammen als *intercept* bezeichnet).     
- $b_1$ gibt die Steigung der Geraden an.  

### Die Kleinst-Quadrat-Linie

Leider ist es in Wirklichkeit selten so, dass die Daten so genau einem linearen Modell entsprechen und es stellt sich die Frage, auf welche Weise eine Gerade konstruiert werden kann, welche die Daten zwar nicht perfekt, aber doch möglichst genau modelliert.  

Betrachten wir noch einmal das Streudiagramm unseres Buchhändlers: Wie würden Sie die Gerade einzeichnen, welche die Daten am ehesten repräsentieren? Welche der vorgeschlagenen Geraden würden Sie als das am besten zutreffende Modell wählen?  

```{r bookstore-lines-fig, fig.align='center', fig.dim=c(9, 6), fig.cap="Welche Gerade ist ein optimales Modell?"}

p <- ggplot(bookstore, aes(x = Studierende, y = Buecher)) +
  geom_point(color = COL[1], size = 2, alpha = .7) +
  ylab("") +
  #geom_smooth(method = "lm", se = FALSE, color = "darkgrey", alpha = .5) +
  theme_grey()

p1 <- p +
  geom_hline(yintercept = 31.5, color = "red") +
  xlab("(a)")

p2 <- p +
  geom_vline(xintercept = 34, color = "red")+
  xlab("(b)")

p3 <- p +
  geom_abline(intercept = 11, slope = 0.6 , color = "red")+
  xlab("(c)")

p4 <- p +
  geom_abline(intercept = 15, slope = 0.55 , color = "red") +
  xlab("(d)")

p5 <- p +
  geom_abline(intercept = 8.933, slope = 0.6864, color = "red")+
  xlab("(e)")  

p6 <- p +
  geom_abline(intercept = 55, slope = -0.6864, color = "red") +
  xlab("(f)")

(p1 | p2 | p3) /
  (p4 | p5 | p6)
```

(a) Die Gerade beschreibt keinen linearen Zusammenhang.   
(b) Die Gerade beschreibt keinen linearen Zusammenhang. 
(c) Mögliches Modell, allerdings liegen mehr Punkte über als unter der Geraden.  
(d) Mögliches Modell, allerdings liegen mehr Punkte unter als über der Geraden.  
(e) Optimales Modell  
(f) Die Gerade beschreibt einen negativen linearen Zusammenhang, das ist Unsinn.  

```{r model_bookstore}
model1 <- lm(Buecher ~ Studierende, data = bookstore)
bookstore$predicted <- predict(model1)
bookstore$residuals <- residuals(model1)
# summary(model1)
```

```{r plot-bookstore-residuals, fig.dim=c(5, 5), fig.align='center'}
ggplot(bookstore, aes(x = Studierende, y = Buecher)) +
  geom_segment(aes(xend = Studierende, yend = predicted), color = "orange", alpha = 1) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgrey") +
  geom_point(aes(y = predicted), color = COL[4], size = 2) +
  theme_grey()
```

Die Abbildung \@ref(fig:plot-bookstore-residuals) zeigt die Gerade (e), die den funktionalen Zusammenhang zwischen Studierenden und verkauften Büchern modelliert. Die blauen Punkte sind die Anzahl effektiv verkaufter Bücher pro Studierendenanzahl und die roten Punkte sind die Anzahl der Bücher, die unser Modell vorhersagt. Die roten Punkte sind die vom Modell berechneten Werte *(fitted values)* und wir sehen, dass die Gerade die meisten blauen Punkte verfehlt. Die senkrechten Linien zwischen den gemessenen Werten (blau) und den gefitteten Werten (rot) bezeichnen wir als **Residuen**.  

Definition **Residuum**: Senkrechte Differenz zwischen dem Modell vorhergesagten Wert $\hat{y}$ (gefitteter Wert) und dem tatsächlich beobachteten Wert $y$. Das Residuum $e_i$ der i-ten Beobachtung ist die Differenz zwischen dem beobachteten Wert $y_i$ und dem vom Modell vorhergesagten Wert $\hat{y_i}$.

$$e_i = y_i-\hat{y_i}$$  

$\hat{y_i}$ berechnen wir durch Einsetzen von $x_i$ in die Regressionsgleichung.   

Nun stellt sich die Frage: Was ist ein objektives Mass um die beste Gerade zu finden? Aus mathematischer Sicht möchten wir eine Linie, die möglichst kleine Residuen ergibt. Die erste Option ist, eine Linie zu finden, bei der die Summe der Absolutwerte der Residuen minimal ist:

$$|e_1| + |e_2| + ... + |e_n|=minimal$$

Dies wäre eine durchaus eine Möglichkeit, die üblichere Praxis ist allerdings, dass eine Gerade berechnet wird, bei der die **Summe der quadrierten Residuen** minimal ist (sog. Kleinst-Quadrat-Methode):   

$$e_1^2 + e_2^2 + ... + e_n^2 = minimal$$  

Vorteile der Kleinst-Quadrat-Methode  

1. Es ist die übliche Methode.   
2. Jede Statistiksoftware berechnet die Regressionsgerade standardmässig mit dieser Methode.  
3. Durch das Quadrieren erhalten grosse Abweichungen ein stärkeres Gewicht als kleine Abweichungen; diese Methode "bestraft" das Modell, wenn grosse Abweichungen vorkommen.   

Die ersten beiden Gründe sind reine Konvention, der letzte Grund rechtfertigt die Methode jedoch aus mathematischer Sicht.  

```{r elmhurst-fig, fig.dim = c(5, 5), fig.align='center', fig.cap="Universitäre Unterstützung nach Familieneinkommen"}
d <- openintro::elmhurst
d$gift_aid <- d$gift_aid * 1000
d$family_income <- d$family_income * 1000

g <- lm(d$gift_aid ~ d$family_income)
# summary(g)

loss <- function(a, b, d) {
  p <- a + b * d$family_income
  sum(abs(d$gift_aid - p))
}
a      <- round(g$coef[1], 2) + seq(-500, 500, 1)
b      <- round(g$coef[2], 3) + seq(-0.01, 0.01, 0.0001)
mins   <- c(a[1], b[1])
theMin <- loss(a[1], b[1], d)
pb     <- txtProgressBar(1, length(a), style=3)
for (i in 1:length(a)) {
  for (j in 1:length(b)) {
    hold <- loss(a[i], b[j], d)
    if (hold < theMin) {
      mins <- c(a[i],b[j])
      theMin <- hold
    }
  }
}

BuildElmhurtPlot <- function() {
  plot(d$family_income, d$gift_aid,
      col = COL[1, 2],
      pch = 19,
      xlab = 'Familieneinkommen',
      ylab = '', axes=FALSE,
      xlim = c(0, 280e3), 
      ylim = c(0, 35e3))
  AxisInDollars(1, at = (0:8) * 50e3)
  AxisInDollars(2, at = (0:3) * 10e3)
  box()
  par(las = 0)
  mtext("Unterstützung durch Universität", 2, line = 3)
}

BuildElmhurtPlot()
abline(mins[1], mins[2], lty=2, lwd=2)
abline(g, lwd = 2)
```

Die Abbildung \@ref(fig:elmhurst-fig) zeigt die finanzielle Unterstützung durch die Universität Elmhurst (Illinois) in Abhängigkeit vom Familieneinkommen [Quelle @R-openintro]. Die gestrichelte Linie wurde mit der Methode der Summe der Absolutwerte der Residuen und die ausgezogene Linie mit der Kleinst-Quadrat-Methode berechnet. Beide Varianten würden ein plausibles Modell ergeben, aber die Kleinst-Quadrat-Methode hat sich als Standard etabliert.   

Die Berechnung der Geradengleichung überlassen wir i.d.R. `R`. Hier die Formeln für die Berechnung von Hand für die einfache lineare Regression:

**Berechnung der Steigung**

$$\beta_1 = \frac{s_y}{s_x}r$$ 
wobei $s_y$ die Standardabweichung von $y$, $s_x$ die Standardabweichung von $y$ und $r$ der Korrelationskoeffizient nach Pearson ist.  

**Berechnung des Achsenabschnitts**  

$$\beta_0 = \bar{y} - \beta_1 \bar{x}$$  
wobei $\bar{y}$ der Mittelwert von $y$ und $\bar{x}$ der Mittelwert von $x$ ist.  

in `R` benutzen wir die Funktion `lm()`:

```{r, echo=TRUE}
### R-Code

# lineares Modell erstellen und Zusammenfassung ausgeben
lm_bookstore <- lm(Buecher ~ Studierende, data = bookstore)
summary(lm_bookstore)
```

```{r, eval=FALSE}
library(equatiomatic)
eq_bookstore <- extract_eq(lm_bookstore, intercept = "beta", use_coefs = TRUE)
```

Für unser Beispiel berechnet `R` für die Koeffizienten     

- $\beta_0$ = 8.93 (Intercept)      
- $\beta_1$ = 0.69   

eingesetzt in die lineare Gleichung resultiert 

$$\widehat{Buecher} = 8.93 + 0.69 \times Studierende$$


**Interpretation**   

- $\beta_0$ Achsenabschnitt: wenn keine Studierenden den Statistikkurs belegen (x = 0)   
- $\beta_1$ Steigung: Pro zusätzliche Studierende steigt der Buchverkauf um 0.69 Bücher.   

Unser Modell ermöglicht uns, eine Vorhersage zu machen für Werte, die wir so bisher noch gar nicht beobachtet haben. Wir können jetzt die Anzahl Bücher schätzen, die der Buchhändler im nächsten Semester, in dem 33 Studierende eingeschrieben sind, verkaufen wird, indem wir die Zahl 33 für x einsetzen.

$$\widehat{Buecher} = 8.93 + 0.69 \times 33 = 31.7$$ 

Unser Modell sagt voraus, dass der Buchhändler 32 (31.7) Statistikbücher im nächsten Semester verkaufen wird.  

```{r bookstore-33-fig, fig.dim=c(5, 5), fig.align='center', fig.cap="Buchhandlung: Vorhersage für 33 Stud."}
y_hat_33 <- 8.93 + 0.69 * 33

ggplot(bookstore, aes(x = Studierende, y = Buecher)) +
  #geom_segment(aes(xend = Studierende, yend = predicted), color = "orange", alpha = 1) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgrey") +
  #geom_point(aes(y = predicted), color = COL[4], size = 2) +
  geom_point(aes(y = y_hat_33, x = 33), color = COL[2], size = 5) +
  geom_segment(aes(x = 33, xend = 33, y = 25.5, yend = y_hat_33), color = COL[2]) +
  geom_segment(aes(x = 33, xend = 26, y = y_hat_33, yend = y_hat_33), color = COL[2], arrow = arrow()) +
  geom_text(aes(x = 33, y = 25, label = "33"), color = COL[2], size = 5) +
  theme_grey()
```


### Warum man nicht über die gemessenen Daten hinaus extrapolieren sollte

Die Regressionsgleichung ist nur für den gemessenen Datenbereich gültig. Was dabei herauskommt, wenn man die Vorhersage über den gemessenen Datenbereich hinaus extrapoliert zeigt das folgende Beispiel.   

Im September 2004 publizierte das Magazin [Nature](https://www.nature.com/articles/431525a#MOESM1) einen Artikel, in dem die Entwicklung der olympischen Siegerzeiten über 100m Sprint zwischen Männern und Frauen seit 1900 verglichen wurden. Die Autoren machten die Vorhersage, dass im Jahre 2156 die Frauen die 100m-Distanz schneller laufen werden als die Männer.  

```{r sprint100m-fig, fig.cap="Olympische Zeit für 100m-Sprint"}
# load dataset
# source: https://static-content.springer.com/esm/art%3A10.1038%2F431525a/MediaObjects/41586_2004_BF431525a_MOESM1_ESM.doc
# https://www.nature.com/articles/431525a#MOESM1
sprint100m <- read_csv("./data/06_sprint100.csv")

# calculate multiple regression model
model_sprint100m <- lm(win_sec ~ year + sex, data = sprint100m)
# summary(model_sprint100m)


# calculate separate models for sex
sprint_m <- sprint100m %>% filter(sex == "m")
sprint_f <- sprint100m %>% filter(sex == "f")

mod_m <- lm(win_sec ~ year, data = sprint_m)
mod_f <- lm(win_sec ~ year, data = sprint_f)

# coef(mod_m)
# coef(mod_f)

annotate_mod_m <- paste("y =", round(coef(mod_m)[1], 2), round(coef(mod_m)[2], 2), "x")
annotate_mod_f <- paste("y =", round(coef(mod_f)[1], 2), round(coef(mod_f)[2], 2), "x")

# plot data, split by sex
ggplot(sprint100m, aes(x = year, y = win_sec)) +
  geom_point(aes(color = sex)) +
  geom_smooth(aes(color = sex), method = "lm", se = FALSE) +
  geom_text(x = 1920, y = 10.2, label = annotate_mod_m, color = ggblue) +
  geom_text(x = 1980, y = 11.5, label = annotate_mod_f, color = ggred) + 
  ylab("sec") +
  xlab("Jahr") +
  ggtitle("Olympische Zeit für 100m-Sprint (1900 bis 2004)") +
  theme_grey()
```

Die Steigung bei den Frauen ist etwas stärker negativ als bei den Männern ($\beta_{1, f}$ = -0.02, $\beta_{1,m}$ = -0.01). Das heisst, dass die Frauen in den Jahren 1928 bis 2004 ihre Geschwindigkeit im Durchschnitt um 0.01 Sekunden mehr steigern konnten als die Männer: Frauen wurden im Durchschnitt pro Jahr um 0.02 Sekunden schneller, Männer um 0.01 Sekunden. 

```{r sprint100m-extrapol-fig, fig.cap="Zeit für 100m-Sprint Frauen extrapoliert"}
# extrapolate for years 2004 to 2200
new_years <- tibble(year = seq(2004, 2200, by = 4))
pred_m <- predict(mod_m, new_years)
pred_f <- predict(mod_f, new_years)

pred_sprint <- tibble(year = c(new_years$year, new_years$year), 
                      win_sec = c(pred_m, pred_f), 
                      sex = c(rep("m", 50), rep("f", 50)))

sprint_extrapol <- bind_rows(sprint100m, pred_sprint)
# write_csv(sprint_extrapol, file = "./data/06_sprint100_predict.csv")

ggplot(sprint100m, aes(x = year, y = win_sec, color = sex)) +
  geom_point() +
  geom_smooth(data = sprint_extrapol, aes(x = year, y = win_sec, color = sex), method = "lm", se = FALSE) +
  geom_segment(x = 2156, xend = 2156, y = 7.2, yend = 8.098, arrow = arrow(), col = COL[2]) +
  geom_text(aes(x = 2156, y = 7, label = "2156"), color = COL[2], size = 5) +
  theme_grey() +
  ylab("sec") +
  xlab("Jahr") +
  ggtitle("Zeit für 100m-Sprint Frauen extrapoliert", subtitle = "Ab 2156 überholen Frauen die Männer beim 100m-Sprint")
```

Die Autoren des Artikels haben nun berechnet, wie sich die Laufzeiten in Zukunft entwickeln werden. Extrapoliert man die Regressionsgleichung der olympischen Zeiten für 100m-Sprint über den Messzeitraum hinaus, überschneiden sich die Regressionsgeraden im Jahr 2156. Ab dann überholen die Frauen die Männer!   

Man kann die Extrapolation jedoch noch weiter treiben: Wenn die Regressionsgleichung über den Messzeitraum hinaus gültig wäre, würden Frauen ab dem Jahr 2637 weniger als 0 Sekunden für den 100m-Lauf benötigen und in der Zeit zurück reisen!   

```{r sprint100m-zero-fig, fig.cap="Zeit für 100m-Sprint Frauen bis ins Jahr 2800 extrapoliert"}
# Ab wann sind Frauen schneller als 0 sec/100m
# summary(mod_f)
zero_year <- (0 - coef(mod_f)[1])/coef(mod_f)[2]

zero_year_pred <- tibble(year = seq(2000, 2800, by = 100))
zero_year_f <- predict(mod_f, zero_year_pred)
pred_zero_year <- tibble(year = zero_year_pred$year,
                         win_sec = zero_year_f,
                         sex = "f")
sprint_zero <- bind_rows(sprint100m[sprint100m$sex == "f", ], pred_zero_year)

ggplot(sprint_zero, aes(x = year, y = win_sec)) +
  geom_point(data = sprint_zero[sprint_zero$year < 2100, ], color = ggred) +
  geom_smooth(method = "lm", se = FALSE, color = ggred) +
  geom_hline(yintercept = 0) +
  geom_segment(x = zero_year, xend = zero_year, y = -1.8, yend = 0, arrow = arrow(), col = COL[2]) +
  geom_text(aes(x = zero_year, y = -2.2, label = "2637"), color = COL[2], size = 5) +
  theme_grey() +
  ylab("sec") +
  xlab("Jahr") +
  ggtitle("Zeit für 100m-Sprint Frauen extrapoliert", subtitle = "Ab 2637 reisen Frauen beim 100m-Sprint in der Zeit zurück")
# zero_year
```

Wir lernen daraus, dass die Regressionsgerade immer nur für den effektiv gemessenen Datenbereich gültig ist.

### Das Bestimmtheitsmass $R^2$   

Das Bestimmtheitsmass $R^2$ ...  

- sagt uns, wieviel Prozent der Streuung der abhängigen Variable $y$ durch die unabhängige Variable $x$ erklärt wird. Anders formuliert: Wieviel Prozent der Änderung in $y$ von einem Datenpunkt zum anderen durch $x$ erklärbar ist.  
- kann Werte zwischen 0 und 1 annehmen (0 bis 100%).  
- ist bei der einfachen linearen Regression das Quadrat des Korrelationskoeffizienten nach Pearson: $R^2 = r^2$.   
- ist umso grösser, je geringer Daten um die Regressionsgerade streuen.   

```{r Rsquared, fig.align='center', fig.dim=c(9, 4)}
set.seed(6825)

d1 <- tibble(
  x = seq(0, 20, by = 1) + rnorm(mean = 5, sd = 3, n = 21),
  y = 5 + 0.3 * x + rnorm(mean = 3, sd = 3, n = 21)
)

library(ggpmisc)
my.formula <- y ~ x
p1 <- ggplot(d1, aes(x = x, y = y)) +
  geom_point(color = COL[1], alpha = .7, size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgrey", fomula = my.formula) +
  stat_poly_eq(fomula = my.formula,
                aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                parse = TRUE) +
  theme_grey()+
  xlab("(a)")

d2 <- tibble(
  x = seq(0, 20, by = 1) + rnorm(mean = 5, sd = 3, n = 21),
  y = 5 + 0.3 * x + rnorm(mean = 1.5, sd = 1, n = 21)
)
p2 <- ggplot(d2, aes(x = x, y = y)) +
  geom_point(color = COL[1], alpha = .7, size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgrey", fomula = my.formula) +
  stat_poly_eq(fomula = my.formula,
                aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                parse = TRUE)+
  theme_grey() +
  xlab("(b)")
  

d3 <- tibble(
  x = seq(0, 20, by = 1) + rnorm(mean = 5, sd = 3, n = 21),
  y = 5 + 0.3 * x + rnorm(mean = .5, sd = .2, n = 21)
)
p3 <- ggplot(d3, aes(x = x, y = y)) +
  geom_point(color = COL[1], alpha = .7, size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgrey", fomula = my.formula) +
  stat_poly_eq(fomula = my.formula,
                aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                parse = TRUE)+
  theme_grey()+
  xlab("(c)")

(p1 | p2 | p3)
```

Eine Variable $y$ kann folglich umso besser durch die Variable $x$ erklärt werden, je größer die Korrelation $r$ zwischen beiden Variablen dem Betrag nach ist. Anders formuliert: Je größer $R^2$ ist, desto besser ist die Anpassung der Regressionsgeraden an die Daten.   



## Einfache lineare Regression in `R/jamovi`

### `R` Modell erstellen und Output interpretieren

Zur Erläuterung des R-Outputs verwenden wir die Fragestellung aus der Buchhandlung. Es werden nur die wichtigsten Angaben erläutert. Eine detaillierte Beschreibung des Outputs findet man z.B. [hier](https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R)

```{r, eval=FALSE}
### R-Code

# ein lineares Modell berechnen
model <- lm(Buecher ~ Studierende, data = bookstore)

# Zusammenfassung des Modells anzeigen
summary(model)
```

![R Output Einfaches lineares Modell](./illustrations/06_lm_R_output.png)

**Erläuterung zum Nullhypothesentest für die Koeffizienten**    

Es interessiert die Frage, ob die Steigung von 0 verschieden ist (Steigung = 0 bedeutet kein Zusammenhang). Die Hypothesen zu dieser Frage sind:   

- $H_0: \beta_1 = 0$   
- $H_A: \beta_1 \neq 0$   

Das Statistikprogramm führt immer einen t-Test für eine einfache Stichprobe durch und ermittelt den $t$-Wert und den $p$-Wert für die Koeffizienten. Ist der $p$-Wert kleiner als das  Signifikanzniveau, das meist auf $\alpha$ < 0.05 festgelegt wird, liegt Evidenz dafür vor, dass die Nullhypothese, dass kein Zusammenhang zwischen abhängiger und unabhängiger Variable vorlieg, verworfen werden kann. 

Meist interessiert nur der $p$-Wert für die Steigung $b_1$. Im vorliegenden Fall ist $p < 0.001$, was uns schlussfolgern lässt, dass ein signifikanter Zusammenhang zwischen der Anzahl verkaufter Bücher und der Anzahl Studierenden vorliegt.

### `jamovi` Modell erstellen und Output interpretieren

`jamovi\Analyses\Regression\Linear Regression`   

- Dependent Variable: `Bücher`  
- Covariates: `Studierende` 

![R Output Einfaches lineares Modell](./illustrations/06_jamovi_lm_output.png)
 
## Voraussetzungen für die Gültigkeit des linearen Modells   

1. Die Residuen sind voneinander unabhängig (schwierig zu prüfen, abhängig vom Studiendesign)   
2. Die Residuen sind normalverteilt mit dem Mittelwert 0 und der Streuung $\sigma$.   
3. Die Streuung $\sigma$ der Residuen ist über den Bereich von x konstant (Homoskedastizität).   

**Achtung: Die lineare Regression macht keinerlei Annahmen bezügliche der Verteilung der Variablen selber! die Annahme einer Normalverteilung bezieht sich nur auf die Residuen!**    

### Diagnostische Plots

Die Verteilung der Residuen wird anhand eines QQ-Plots der Residuen geprüft. Die Streung der Residuen anhand eines Plots, der die gefitteten Werte auf der x-Achse und die Residuen auf der y-Achse darstellt.

In `R` kann über die Funktion `plot(model)` eine Serie von 4 diagnostischen Plots 
ausgegeben werden. Der erste Plot erlaubt die Beurteilung der Homoskedastizität, der
zweite Plot ist ein QQ-Plot.

```{r, echo=TRUE}
### R-Code

# Modell erstellen, wenn nicht schon gemacht
model <- lm(Buecher ~ Studierende, data = bookstore)

# über 'which' wählen wir die ersten beiden Plots 
plot(model, which = c(1, 2))
```


Im `jamovi`-Dialogfenster zur Linearen Regression kann unter `Assuption Checks` ein Q-Q-Plot der Residuen (Q-Q plot of residuals) gewählt werden. Jamovi erstellt ein Streudiagramm in dem auf der x-Achse die theoretischen Quantile einer Normalverteilung dargestellt werden und auf der y-Achse die Quantile der standardisierten Residuen. 

Weiter kann unter `Assuption Checks` > `Residual Plots` gewählt werden. Jamovi erstellt mehrere Streudiagramme, von denen uns nur das erste interessiert: Auf der y-Achse werden die Residuen dargestellt und auf der x-Achse die gefitteten Werte. Wenn die Punkte gleichmässig verteilt sind, liegt Homoskedastizität vor.

```{r}
jmv::linReg(
  data = bookstore,
  dep = Buecher,
  covs = Studierende,
  blocks = list(
    list(
      "Studierende")),
  refLevels = list(),
  qqPlot = TRUE,
  resPlots = TRUE)
```

### Interpretation der Plots:   

- QQ-Plot: Die Punkte sind einigermassen auf der Linie und wir entscheiden auf Normalverteilung der Residuen. (Bei kleinen Stichproben wie im vorliegenden Fall ist die Entscheidung meist nicht ganz sicher möglich)   
- Residuen vs. gefittete Werte: Die Streuung der Residuen wird von links nach rechts, d.h. mit zunehmender geschätzter Zahl an verkauften Büchern grösser. Damit ist die Voraussetzung nicht erfüllt, dass die Streuung der Residuen über den gesamten Messbereich gleich ist und es liegt Heteroskedastizität vor.   

### Was bedeutet es, wenn die Voraussetzungen nicht erfüllt sind?   

- Signifikanztests und Konfidenzintervalle für die Koeffizienten haben nicht die erwarteten Eigenschaften und werden dadurch schlecht interpretierbar.   
- Die Koeffizienten sind immer noch gültig!   


### Beispielplots zu Homo- und Heteroskedastizität

```{r heteroscedasticity, fig.align='center', fig.dim = c(8, 6)}
heterosc <- read_csv("./data/06_heteroscedasticity.csv")

# ggplot(heterosc, aes(x = Population, y = Accidents)) +
#   geom_point(color = COL[1], size = 3, alpha = .7) +
#   geom_smooth(method = "lm", se = FALSE)

mod_heterosc <- lm(Accidents ~ Population, data = heterosc)

library(broom)
mod_heterosc_aug <- augment(mod_heterosc)

resplot4 <- ggplot(mod_heterosc_aug, aes(x = .fitted, y = .std.resid)) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  geom_hline(yintercept = 0, color = "red") +
  theme_grey() +
  xlab("(d)")

# ----------------------------------------------------
# from modern statistics with R, p 304

exdata1 <- data.frame(
x = c(2.99, 5.01, 8.84, 6.18, 8.57, 8.23, 8.48, 0.04, 6.80,
7.62, 7.94, 6.30, 4.21, 3.61, 7.08, 3.50, 9.05, 1.06,
0.65, 8.66, 0.08, 1.48, 2.96, 2.54, 4.45),
y = c(5.25, -0.80, 4.38, -0.75, 9.93, 13.79, 19.75, 24.65,
6.84, 11.95, 12.24, 7.97, -1.20, -1.76, 10.36, 1.17,
15.41, 15.83, 18.78, 12.75, 24.17, 12.49, 4.58, 6.76,
-2.92))

exdata2 <- data.frame(
x = c(5.70, 8.03, 8.86, 0.82, 1.23, 2.96, 0.13, 8.53, 8.18,
6.88, 4.02, 9.11, 0.19, 6.91, 0.34, 4.19, 0.25, 9.72,
9.83, 6.77, 4.40, 4.70, 6.03, 5.87, 7.49),
y = c(21.66, 26.23, 19.82, 2.46, 2.83, 8.86, 0.25, 16.08,
17.67, 24.86, 8.19, 28.45, 0.52, 19.88, 0.71, 12.19,
0.64, 25.29, 26.72, 18.06, 10.70, 8.27, 15.49, 15.58,
19.17))

mod_exdata1 <- lm(y ~  x, data = exdata1)
mod_exdata2 <- lm(y ~  x, data = exdata2)

mod_exdata1_aug <- augment(mod_exdata1)
mod_exdata2_aug <- augment(mod_exdata2)

resplot1 <- ggplot(mod_exdata1_aug, aes(x = .fitted, y = .std.resid)) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  geom_hline(yintercept = 0, color = "red") +
  theme_grey() +
  xlab("(a)")
resplot2 <- ggplot(mod_exdata2_aug, aes(x = .fitted, y = .std.resid)) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  geom_hline(yintercept = 0, color = "red") +
  theme_grey()+
  xlab("(b)")

# ---
# http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/
set.seed(5)

N  = 500
b0 = 3
b1 = 0.4

s2 = 5
g1 = 1.5
g2 = 0.015

x        = runif(N, min=0, max=100)
y_homo   = b0 + b1*x + rnorm(N, mean=0, sd=sqrt(s2            ))
y_hetero = b0 + b1*x + rnorm(N, mean=0, sd=sqrt(exp(g1 + g2*x)))

hh <- tibble(
  x = x,
  y_homo = y_homo,
  y_hetero = y_hetero
)

mod.homo   = lm(y_homo~x)
mod.hetero = lm(y_hetero~x)

mod.homo.aug <- augment(mod.homo)

resplot3 <- ggplot(mod.homo.aug, aes(x = .fitted, y = .std.resid)) +
  geom_point(color = COL[1], size = 3, alpha = .7) +
  geom_hline(yintercept = 0, color = "red") +
  theme_grey()+
  xlab("(c)")

(resplot1 | resplot2)/
  (resplot3 | resplot4)
```

Nur Abb. c) erfüllt die Bedingung für Homoskedastizität. In Abb. a) bilden die Residuen ein Muster, was die Annahme verletzt, dass die Residuen zufällig um die Null-Linie herum streuen. In Abb. b) und d) ist die Verteilung heteroskedastisch; mit zunehmendem x nimmt auch die Streuung der Residuen um die Null-Linie herum zu.  
